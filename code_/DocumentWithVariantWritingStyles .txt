Once upon a time there was an old woman who loved baking gingerbread. She would bake gingerbread cookies, cakes, houses and gingerbread people, all decorated with chocolate and peppermint, caramel candies and colored frosting.

She lived with her husband on a farm at the edge of town. The sweet spicy smell of gingerbread brought children skipping and running to see what would be offered that day.

Unfortunately the children gobbled up the treats so fast that the old woman had a hard time keeping her supply of flour and spices to continue making the batches of gingerbread. Sometimes she suspected little hands of having reached through her kitchen window because gingerbread pieces and cookies would disappear. One time a whole gingerbread house vanished mysteriously. She told her husband, "Those naughty children are at it again. They don't understand all they have to do is knock on the door and I'll give them my gingerbread treats."
One day she made a special batch of gingerbread men because they were extra big. Unfortunately for the last gingerbread man she ran out of batter and he was half the size of the others.

She decorated the gingerbread men with care, each having socks, shirt and pants of different colors. When it came to the little gingerbread man she felt sorry for him and gave him more color than the others. "It doesn't matter he's small," she thought, "He'll still be tasty."
Once upon a time . . . as a merchant set off for market, he asked each of
his three daughters what she would like as a present on his return. The first
daughter wanted a brocade dress, the second a pearl necklace, but the third, 
whose name was Beauty, the youngest, prettiest and sweetest of them all, said
to her father:
   "All I'd like is a rose you've picked specially for me!"
   When the merchant had finished his business, he set off for home. However, 
a sudden storm blew up, and his horse could hardly make headway in the howling
gale. Cold and weary, the merchant had lost all hope of reaching an inn when
he suddenly noticed a bright light shining in the middle of a wood. As he drew
near, he saw that it was a castle, bathed in light.
   "I hope I'll find shelter there for the night," he said to himself. When he
reached the door, he saw it was open, but though he shouted, nobody came to
greet him. Plucking up courage, he went inside, still calling out to attract
attention. On a table in the main hall, a splendid dinner lay already served. 
The merchant lingered, still shouting for the owner of the castle. But no one 
came, and so the starving merchant sat down to a hearty meal.
   Overcome by curiosity, he ventured upstairs, where the corridor led into
magnificent rooms and halls. A fire crackled in the first room and a soft bed 
looked very inviting. It was now late, and the merchant could not resist. He 
lay down on the bed and fell fast asleep. When he woke next morning, an 
unknown hand had placed a mug of steaming coffee and some fruit by his
bedside.
   The merchant had breakfast and after tidying himself up, went downstairs to
thank his generous host. But, as on the evening before, there was nobody in 
sight. Shaking his head in wonder at the strangeness of it all, he went 
towards the garden where he had left his horse, tethered to a tree. Suddenly, 
a large rose bush caught his eye.
   Remembering his promise to Beauty, he bent down to pick a rose. lnstantly, 
out of the rose garden, sprang a horrible beast, wearing splendid clothes. Two
bloodshot eyes, gleaming angrily, glared at him and a deep, terrifying voice 
growled: "Ungrateful man! I gave you shelter, you ate at my table and slept in 
my own bed, but now all the thanks I get is the theft of my favourite flowers!
I shall put you to death for this slight!" Trembling with fear, the merchant 
fell on his knees before the Beast.
   "Forgive me! Forgive me! Don't kill me! I'll do anything you say! The rose 
wasn't for me, it was for my daughter Beauty. I promised to bring her back a 
rose from my journey!" The Beast dropped the paw it had clamped on the unhappy
merchant.
   "I shall spare your life, but on one condition, that you bring me your 
daughter!" The terror-stricken merchant, faced with certain death if he did 
not obey, promised that he would do so. When he reached home in tears, his 
three daughters ran to greet him. After he had told them of his dreadful 
adventure, Beauty put his mind at rest immediately.
   "Dear father, I'd do anything for you! Don't worry, you'll be able to keep 
your promise and save your life! Take me to the castle. I'll stay there in 
your place!" The merchant hugged his daughter.
   "I never did doubt your love for me. For the moment I can only thank you 
for saving my life." So Beauty was led to the castle. The Beast, however, had 
quite an unexpected greeting for the girl. Instead of menacing doom as it had 
done with her father, it was surprisingly pleasant. 
   In the beginning, Beauty was frightened of the Beast, and shuddered at the 
sight of it. Then she found that, in spite of the monster's awful head, her 
horror of it was gradually fading as time went by. She had one of the finest 
rooms in the Castle, and sat for hours, embroidering in front of the fire. And
the Beast would sit, for hours on end, only a short distance away, silently 
gazing at her. Then it started to say a few kind words, till in the end, 
Beauty was amazed to discover that she was actually enjoying its conversation.
The days passed, and Beauty and the Beast became good friends. Then one day, 
the Beast asked the girl to be his wife.                  .-~
   Taken by surprise, Beauty did not know what to say. Marry such an ugly 
monster? She would rather die! But she did not want to  hurt the feelings of 
one who, after all, had been kind to her. And she remembered too that she owed
it her own life as well as her father's.
   "I really can't say yes," she began shakily. "I'd so much like to . . ." 
The Beast interrupted her with an abrupt gesture.
   "I quite understand! And I'm not offended by your refusal!" Life went on as
usual, and nothing further was said. One day, the Beast presented Beauty with 
a magnificent magic mirror. When Beauty peeped into it, she could see her 
family, far away.
   "You won't feel so lonely now," were the words that accompanied the gift. 
Beauty stared for hours at her distant family. Then she began to feel worried.
One day, the Beast found her weeping beside the magic mirror.
   "What's wrong?" he asked, kindly as always.                        
   "My father is gravely ill and close to dying! Oh, how I wish I could see 
him again, before it's too late!" But the Beast only shook its head.
   "No! You will never leave this castle!" And off it stalked in a rage. 
However, a little later, it returned and spoke solemnly to the girl._
   "If you swear that you will return here in seven days time, I'll let you go
and visit your father!" Beauty threw herself at the Beast's feet in delight.
   "I swear! I swear I will! How kind you are! You've made a loving daughter 
so happy!" In reality, the merchant had fallen ill from a broken heart at 
knowing his daughter was being kept prisoner. When he embraced her again, he 
was soon on the road to recovery. Beauty stayed beside him for hours on end, 
describing her life at the Castle, and explaining that the Beast was really 
good and kind. The days flashed past, and at last the merchant was able to 
leave his bed. He was completely well again. Beauty was happy at last. 
However, she had failed to notice that seven days had gone by.
   Then one night she woke from a terrible nightmare. She had dreamt that the 
Beast was dying and calling for her, twisting in agony. 
   "Come back! Come back to me!" it was pleading. The solem  promise she had 
made drove her to leave home immediately. 
   "Hurry! Hurry, good horse!" she said, whipping her steed onwards towards 
the castle, afraid that she might arrive too late. She rushed up the stairs, 
calling, but there was no reply. Her heart in her mouth, Beauty ran into the 
garden and there crouched the Beast, its eyes shut, as though dead. Beauty 
threw herself at it and hugged it tightly.
   "Don't die! Don't die! I'll marry you . . ." At these words, a miracle took
place. The Beast's ugly snout turned magically into the face of a handsome 
young man.
    "How I've been longing for this moment!" he said. "I was suffering in 
silence, and couldn't tell my frightful secret. An evil witch turned me into a
monster and only the love of a maiden willing to accept me as I was, could 
transform me back into my real self. My dearest! I'll be so happy if you'll 
marry me . . ."
   The wedding took place shortly after and, from that day on, the young 
Prince would have nothing but roses in his gardens. And that's why, to this 
day, the castle is known as the Castle of the Rose.
 Mr. Rabbit sat on his front porch rocking, eating a great big carrot, 
and looking.

    "Looks like Sly Fox coming down the road," he said to himself, walking 
to the end of the porch. Shading his eyes with his paws, he exclaimed, "It 
is Sly Fox." 

    "Good morning Mr. Rabbit," cried Sly Fox, as he walked across the yard. 
"Good morning," replied Mr. Rabbit, a slight frown on his face. 

    "Well," said Sly Fox, "as I haven't seen you in so long a time, thought 
I would stop and chat a while." 

    Mr. Rabbit could not be rude in his own home, even to an enemy, so he 
offered Sly Fox a seat on the porch. 

    "Take a chair," he said politely.   But Sly Fox did not stay long, and as 
he was leaving, he asked: "Mr. Rabbit, my mother is having a good dinner 
tonight. Won't you, Mrs. Rabbit, and your three little rabs come to dinner 
with me?" 

    Oh, thought Mr. Rabbit, he knows about my little rabs and wants to take 
us off to eat us. He pretended to be disappointed as he replied: "Sorry, Sly 
Fox, we have an engagement for today, but if you want us we can come 
tomorrow." 

    At this Sly Fox chuckled inwardly, and readily agreed to come for them 
the next day. Wishing Mr. Rabbit "Good day", he trotted on down the road 
toward his home. 

    As soon as he was out of sight, Mr. Rabbit ran into his house and called 
Mrs. Rabbit. "Get all our things together," he said, "and put rubber boots 
on our little rabs. We have to move quickly to the Piney Woods across the 
brook. Old Sly Fox has found our home and will destroy us." 

    In no time at all the Rabbit family had moved, and the little rabs were 
delighted with their new home. A woodland of towering pines it was, the 
ground covered with pine needles which made a soft carpeting. The wind made 
music in the pine trees, birds sang, and the fragrance of flowers filled the 
air. They found a huge hollow tree where Mr. Rabbit burrowed deep and made 
them a cozy home. Squirrels had left nuts hidden around in the old tree. 
Owls hooted throughout the night, crickets chirped merrily. 

    Next morning old Sly Fox knocked on the door where he had left Mr. 
Rabbit. Mrs. Hedgehog answered the door. "Good morning, Mrs. Hedgehog. Is 
Mr. Rabbit in?" inquired Sly Fox with a wicked grin and a cunning look in 
his eyes. 

    "No," replied Mrs. Eedgehog, none too cordially. "The Rabbit family 
moved to parts unknown right after you left yesterday." 

    "Ah," exclaimed old Sly Fox, "Mr. Rabbit and family were going to have 
dinner with me. My mother has planned a real feast. Why don't you come and 
enjoy it with us?" 

    "Oh," replied Mrs. Hedgehog, smacking her lips and thinking of all the 
goodies, "I have just moved in and there is so much to do! Why not let it go 
until tomorrow?" 

    "Do you like nice young grasshoppers?" asked Sly Fox softly. 

    "Do I? Nothing so good as tender young grasshoppers," answered Mrs. 
Hedgehog, fairly dribbling at the mouth at the thought of such a dainty. 

    "Well," said Sly Fox, "we pass a field where there are any number of 
them. Come get in this sack, and when I stop in the field we will open the 
sack and rake in all of them we want. Mother will bake them with apples and 
they will be deilicious!" This was too much for greedy Mrs. Hedgehog to 
resist, so in the sack she went. Sly Fox with a grin grabbed the sack, threw 
it over his shoulder and trotted toward home.     After going a long way, Mrs. 
Hedgehog became suspicious and cried, "How long before we reach that field 
of grasshoppers?" 

    "Why, you silly, greedy hedgehog, there is no field of grasshoppers for 
you. I am going to eat you for my dinner. It's you with apple dumplings that 
my mother will bake." 

    Every hair on Mrs. Hedgehog's head stood on end with fright. Oh, how 
foolish she had been! Her greed had trapped her. If only she had stayed home 
and straightened her house and cooked her own dinner, she would not have 
been in this sack to be eaten by Sly Fox. Greediness never pays, she thought 
to herself. 

    Sly Fox became tired, and as a slight rain had begun to fall, he looked 
for a dry place to sit down. Throwing the sack to the ground and chuckling 
at the thought of sitting on Mrs. Hedgehog, he dropped heavily upon the 
sack. 

    "Wow, Wow!" he cried, jumping quickly up, for Mrs. Hedgehog shot her 
sharp quills into him with all her might. 

    Sly Fox ran to and fro trying to pull out the quills, but they had gone 
too deep. Home he ran, screaming to his mother. Old Mother Fox threw him 
over a log and began pulling out the quills, at the same time calling to a 
neighbor fox to bring some honey to put on the places where the quills had 
been. 

    Mrs. Hedgehog crawled out of the bag and began walking slowly toward 
home. She thought to herself that never again would she be so greedy and 
allow herself to be fooled by Sly Fox or any one else. 

    Meanwhile, Mr. Rabbit and family were living happily in Piney Woods. The 
little rabs played on the crystal clear brook that ran through the woods, 
wading, sailing little leaf boats, and trying to catch the silvery minnows 
darting here and there. 

    Late one evening Papa and Mama Rabbit were sitting before the cozy fire 
talking. Papa Rabbit had on his house robe and bedroom slippers, reading the 
newspaper. Every now and then he looked over his spectacles lovingly at 
dainty little Mama Rabbit, dressed in a flowered housecoat and red slippers 
and knitting little socks for the little rabs. 

    "Sniff! Sniff! Sniff!" came suddenly to their ears. 

    "Sly Fox!" whispered Papa Rabbit, his face now full of concern and 
alarm. 

    "Yes," agreed Mama Rabbit, her voice trembling with fright. 

    "Go cover the little rabs with straw and tell them to be very, very 
quiet," instructed Papa Rabbit. 

    Mrs. Rabbit quickly covered the little rabs and cautioned them to be as 
quiet as mice. Since they were well behaved and obedient little rabs, they 
did just as their mother told them. 

    "I left my big stick beside the old oak tree," cried Papa Rabbit under 
his breath. "What shall we do?" 

    "Sniff! Sniff! Sniff!" went Sly Fox again, scratching up the earth by 
the old hollow tree as he began to dig furiously. The poor little Rabbit 
family sat still and frightened, their hearts thumping, their paws shaking, 
and their eyes bulging with panic. Suddenly in the distance they heard the 
"Toot! Toot!Toot!" of horns, and the "Woof! Woof! Woof!" of barking dogs. 

    Papa Rabbit whispered, "Fox hunters!" as his heart gave a bound of 
relief. 

    Nearer and nearer came the baying of the hounds and the music of the 
horns. Old Sly Fox was so busily digging that he failed to hear at first, 
but suddenly he stopped digging, and threw back his ears to listen. Then he 
quickly jumped away from the log where the Rabbit family lived and started 
running. 

    But the hounds were right after him, baying loudly with all their might. 
The horses' feet beat out an excited rhythm as the red-coated fox hunters 
urged them on in the chase. Up hill, over the meadows they ran. 

    Sly Fox was now running for his life, but the dogs were getting closer 
and closer. He jumped across the brook and spied a hole among some bushes. 
Into this he slid, and as the dogs went down the side of the stream of water 
before they jumped across they lost his scent. Sly Fox quickly ran out of 
the hole and took off in the opposite direction from the way the dogs were 
going. He had been so frightened and so near death that he resolved to 
himself never to bother the Rabbit family again. 

    Meanwhile, when Papa Rabbit had heard the hounds start the chase, he 
turned to Mama Rabbit and cried, "Safe at last! Call our little rabs for 
prayers of thanksgiving and praise to our Father which art in heaven." 

    After prayers, Mama Rabbit hustled about making mint tea for her and 
Papa Rabbit, and hot chocolate piled high with whipped cream for the little 
rabs. After that time they Lived happily among the great whispering pines, 
never bothered by old Sly Fox.
Once upon a time . . . a woodcutter lived happily with his wife in a pretty
little log cabin in the middle of a thick forest. Each morning he set off 
singing to work, and when he came home in the evening, a plate of hot steaming 
soup was always waiting for him. 
   One day, however, he had a strange surprise. He came upon a big fir tree 
with strange open holes on the trunk. It looked somehow different from the 
other trees, and just as he was about to chop it down, the alarmed face of an 
elf popped out of a hole.
   "What's all this banging?" asked the elf. "You're not thinking of cutting 
down this tree, are you? It's my home. I live here!" The woodcutter dropped 
his axe in astonlshment.
   "Well, I . . ." he stammered.
   "With all the other trees there are in this forest, you have to pick this 
one. Lucky I was in, or I would have found myself homeless."
   Taken aback at these words, the woodcutter qulckly recovered, for after all
the elf was quite tiny, while he himself was a big hefty chap, and he boldly 
replied: "I'll cut down any tree I like, so . . ." 
   "All right! All right!" broke in the elf. "Shall we put it this way: if you
don't cut down this tree, I grant you three wishes. Agreed?" The woodcutter 
scratched his head. 
   "Three wishes, you say? Yes, I agree." And he began to hack at another 
tree. As he worked and sweated at his task, the woodcutter kept thinking about
the magic wishes. 
   "I'll see what my wife thinks..."
   The woodcutter's wife was busily cleaning a pot outside the house when her 
husband arrived. Grabbing her round the waist, he twirled her in delight.
   "Hooray! Hooray! Our luck is in!" 
   The woman could not understand why her husband was so pleased with himself 
and she shrugged herself free. Later, however, over a glass of fine wine at 
the table, the woodcutter told his wife of his meeting with the elf, and she 
too began to picture the wonderful things that the elf's three wishes might 
give them. The woodcutter's wife took a first sip of wine from her husband's 
glass.
   "Nice," she said, smacking her lips. "I wish I had a string of sausages to 
go with it, though..."
   Instantly she bit her tongue, but too late. Out of the air appeared the 
sausages while the woodcutter stuttered with rage.
   ". . . what have you done! Sausages . . . What a stupid waste of a wish! 
You foollsh woman. I wish they would stick up your nose!" No sooner sald than
done. For the sausages leapt up and stuck fast to the end of the woman's nose.
This time, the woodcutter's wife flew into a rage.
   "You idiot, what have you done? With all the things we could have wished 
for . . ." The mortified woodcutter, who had just repeated his wife's own 
mistake, exclaimed:
   "I'd chop . . ." Luckily he stopped himself in time, realizing with horror 
that he'd been on the point of having his tongue chopped off. As his wife 
complained and blamed him, the poor man burst out laughing.
   "If only you knew how funny you look with those sausages on the end of your
nose!" Now that really upset the woodcutter's wife. She hadn't thought of her 
looks. She tried to tug away the sausages but they would not budge. She pulled 
again and again, but in vain. The sausages were firmly attached to her nose. 
Terrified, she exclaimed: "They'll be there for the rest of my life!"
   Feeling sorry for his wife and wondering how he could ever put up with a 
woman with such an awkward nose, the woodcutter said: "I'll try." Grasping the
string of sausages, he tugged with all his might. But he simply pulled his 
wife over on top of him. The pair sat on the floor, gazing sadly at each other.
   "What shall we do now?" they said, each thinking the same thought.
   "There's only one thing we can do . . ." ventured the woodcutter's wife 
timidly.
   "Yes, I'm afraid so . . ." her husband sighed, remembering their dreams of 
riches, and he bravely wished the third and last wish "I wish the sausages 
would leave my wife's nose."
   And they did. Instantly, husband and wife hugged each other tearfully, 
saying "Maybe we'll be poor, but we'll be happy again!"
   That evening, the only reminder of the woodcutter's meeting with the elf 
was the string of sausages. So the couple fried them, gloomily thinking of 
what that meal had cost them.
 

 Authorship attribution supported by statistical or computational methods has a long history
starting from 19th century and marked by the seminal study of Mosteller and Wallace (1964)
on the authorship of the disputed Federalist Papers. During the last decade, this scientific field
has been developed substantially taking advantage of research advances in areas such as
machine learning, information retrieval, and natural language processing. The plethora of
available electronic texts (e.g., e-mail messages, online forum messages, blogs, source code,
etc.) indicates a wide variety of applications of this technology provided it is able to handle
short and noisy text from multiple candidate authors. In this paper, a survey of recent
advances of the automated approaches to attributing authorship is presented examining their
characteristics for both text representation and text classification. The focus of this survey is
on computational requirements and settings rather than linguistic or literary issues. We also
discuss evaluation methodologies and criteria for authorship attribution studies and list open
questions that will attract future work in this area. 
The main idea behind statistically or computationally-supported authorship attribution is that
by measuring some textual features we can distinguish between texts written by different
authors. The first attempts to quantify the writing style go back to 19th century, with the
pioneering study of Mendenhall (1887) on the plays of Shakespeare followed by statistical
studies in the first half of the 20th century by Yule (1938; 1944) and Zipf (1932). Later, the
detailed study by Mosteller and Wallace (1964) on the authorship of ‘The Federalist Papers’
(a series of 146 political essays written by John Jay, Alexander Hamilton, and James
Madison, twelve of which claimed by both Hamilton and Madison) was undoubtedly the most
influential work in authorship attribution. Their method was based on Bayesian statistical
analysis of the frequencies of a small set of common words (e.g., ‘and’, ‘to’, etc.) and
produced significant discrimination results between the candidate authors.
Essentially, the work of Mosteller and Wallace (1964) initiated non-traditional
authorship attribution studies, as opposed to traditional human expert-based methods. Since
then and until the late 1990s, research in authorship attribution was dominated by attempts to
define features for quantifying writing style, a line of research known as ‘stylometry’
(Holmes, 1994; Holmes, 1998). Hence, a great variety of measures including sentence length,
word length, word frequencies, character frequencies, and vocabulary richness functions had
been proposed. Rudman (1998) estimated that nearly 1,000 different measures had been
proposed that far. The authorship attribution methodologies proposed during that period were
computer-assisted rather than computer-based, meaning that the aim was rarely at developing
a fully-automated system. In certain cases, there were methods achieved impressive
preliminary results and made many people think that the solution of this problem was too
close. The most characteristic example is the CUSUM (or QSUM) technique (Morton &
Michealson, 1990) that gained publicity and was accepted in courts as expert evidence.
However, the research community heavily criticized it and considered it generally unreliable
(Holmes & Tweedie, 1995). Actually, the main problem of that early period was the lack of 
2
objective evaluation of the proposed methods. In most of the cases, the testing ground was
literary works of unknown or disputed authorship (e.g., the Federalist case), so the estimation
of attribution accuracy was not even possible. The main methodological limitations of that
period concerning the evaluation procedure were the following:
• The textual data were too long (usually including entire books) and probably not
stylistically homogeneous.
• The number of candidate authors was too small (usually 2 or 3).
• The evaluation corpora were not controlled for topic.
• The evaluation of the proposed methods was mainly intuitive (usually based on
subjective visual inspection of scatterplots).
• The comparison of different methods was difficult due to lack of suitable
benchmark data.
Since the late 1990s, things have changed in authorship attribution studies. The vast
amount of electronic texts available through Internet media (emails, blogs, online forums, etc)
increased the need for handling this information efficiently. This fact had a significant impact
in scientific areas such as information retrieval, machine learning, and natural language
processing (NLP). The development of these areas influenced authorship attribution
technology as described below:
• Information retrieval research developed efficient techniques for representing and
classifying large volumes of text.
• Powerful machine learning algorithms became available to handle multidimensional
and sparse data allowing more expressive representations. Moreover,
standard evaluation methodologies have been established to compare different
approaches on the same benchmark data.
• NLP research developed tools able to analyze text efficiently and providing new
forms of measures for representing the style (e.g., syntax-based features).
More importantly, the plethora of available electronic texts revealed the potential of
authorship analysis in various applications (Madigan, Lewis, Argamon, Fradkin, & Ye, 2005)
in diverse areas including intelligence (e.g., attribution of messages or proclamations to
known terrorists, linking different messages by authorship) (Abbasi & Chen, 2005), criminal
law (e.g., identifying writers of harassing messages, verifying the authenticity of suicide
notes) and civil law (e.g., copyright disputes) (Chaski, 2005; Grant, 2007), computer forensics
(e.g., identifying the authors of source code of malicious software) (Frantzeskou, Stamatatos,
Gritzalis, & Katsikas, 2006), in addition to the traditional application to literary research (e.g.,
attributing anonymous or disputed literary works to known authors) (Burrows, 2002; Hoover,
2004a). Hence, (roughly) the last decade can be viewed as a new era of authorship analysis
technology, this time dominated by efforts to develop practical applications dealing with realworld
texts (e.g., e-mails, blogs, online forum messages, source code, etc.) rather than solving
disputed literary questions. Emphasis is now given to the objective evaluation of the proposed
methods as well as the comparison of different methods based on common benchmark
corpora (Juola, 2004). In addition, factors playing a crucial role in the accuracy of the
produced models are examined, such as the training text size (Marton, Wu, & Hellerstein,
2005; Hirst & Feiguina, 2007), the number of candidate authors (Koppel, Schler, Argamon, &
Messeri, 2006), and the distribution of training texts over the candidate authors (Stamatatos,
2008).
In the typical authorship attribution problem, a text of unknown authorship is assigned to
one candidate author, given a set of candidate authors for whom text samples of undisputed
authorship are available. From a machine learning point-of-view, this can be viewed as a
multi-class single-label text categorization task (Sebastiani, 2002). This task is also called
authorship (or author) identification usually by researchers with a background in computer
science. Several studies focus exclusively on authorship attribution (Stamatatos, Fakotakis, &
Kokkinakis, 2001; Keselj, Peng, Cercone, & Thomas, 2003; Zheng, Li, Chen, & Huang, 
3
2006) while others use it as just another testing 
This paper presents a survey of the research advances in this area during roughly the last
decade (earlier work is excellently reviewed by Holmes (1994; 1998)) emphasizing
computational requirements and settings rather than linguistic or literary issues. First, in
Section 2, a comprehensive review of the approaches to quantify the writing style is
presented. Then, in Section 3, we focus on the authorship identification problem (as described
above). We propose the distinction of attribution methodologies according to how they handle
the training texts, individually or cumulatively (per author), and examine their strengths and
weaknesses across several factors. In Section 4, we discuss the evaluation criteria of
authorship attribution methods while in Section 5 the conclusions drawn by this survey are
summarized and future work directions in open research issues are indicated.
2. Stylometric Features
Previous studies on authorship attribution proposed taxonomies of features to quantify the
writing style, the so called style markers, under different labels and criteria (Holmes, 1994;
Stamatatos, Fakotakis, & Kokkinakis, 2000; Zheng, et al., 2006). The current review of text
representation features for stylistic purposes is mainly focused on the computational
requirements for measuring them. First, lexical and character features consider a text as a
mere sequence of word-tokens or characters, respectively. Note that although lexical features
are more complex than character features, we start with them for the sake of tradition. Then,
syntactic and semantic features require deeper linguistic analysis, while application-specific
features can only be defined in certain text domains or languages. The basic feature categories
and the required tools and resources for their measurement are shown in Table 1. Moreover,
various feature selection and extraction methods to form the most appropriate feature set for a
particular corpus are discussed.
2.1 Lexical Features
A simple and natural way to view a text is as a sequence of tokens grouped into sentences,
each token corresponding to a word, number, or a punctuation mark. The very first attempts
to attribute authorship were based on simple measures such as sentence length counts and
word length counts (Mendenhall, 1887). A significant advantage of such features is that they
can be applied to any language and any corpus with no additional requirements except the
availability of a tokenizer (i.e., a tool to segment text into tokens). However, for certain
natural languages (e.g., Chinese) this is not a trivial task. In case of using sentential
information, a tool that detects sentence boundaries should also be available. In certain text
domains with heavy use of abbreviations or acronyms (e.g., e-mail messages) this procedure
may introduce considerable noise in the measures.
The vocabulary richness functions are attempts to quantify the diversity of the
vocabulary of a text. Typical examples are the type-token ratio V/N, where V is the size of the 
4
TABLE 1. Types of stylometric features together with computational tools and resources
required for their measurement (brackets indicate optional tools).
vocabulary (unique tokens) and N is the total number of tokens of the text, and the number of
hapax legomena (i.e., words occurring once) (de Vel, Anderson, Corney, & Mohay, 2001).
Unfortunately, the vocabulary size heavily depends on text-length (as the text-length
increases, the vocabulary also increases, quickly at the beginning and then more and more
slowly). Various functions have been proposed to achieve stability over text-length, including
K (Yule, 1944), and R (Honore, 1979), with questionable results (Tweedie & Baayen, 1998).
Hence, such measures are considered unreliable to be used alone.
The most straightforward approach to represent texts is by vectors of word frequencies. The
vast majority of authorship attribution studies are (at least partially) based on lexical features
to represent the style. This is also the traditional bag-of-words text representation followed by
researchers in topic-based text classification (Sebastiani, 2002). That is, the text is considered
as a set of words each one having a frequency of occurrence disregarding contextual
information. However, there is a significant difference in style-based text classification: the
most common words (articles, prepositions, pronouns, etc.) are found to be among the best
features to discriminate between authors (Burrows, 1987; Argamon & Levitan, 2005). Note
that such words are usually excluded from the feature set of the topic-based text classification
methods since they do not carry any semantic information and they are usually called
‘function’ words. As a consequence, style-based text classification using lexical features
require much lower dimensionality in comparison to topic-based text classification. In other
words, much less words are sufficient to perform authorship attribution (a few hundred
words) in comparison to a thematic text categorization task (several thousand words). More
importantly, function words are used in a largely unconscious manner by the authors and they
are topic-independent. Thus, they are able to capture pure stylistic choices of the authors
across different topics.
The selection of the specific function words that will be used as features is usually based
on arbitrary criteria and requires language-dependent expertise. Various sets of function
words have been used for English but limited information was provided about the way they
have been selected: Abbasi and Chen (2005) reported a set of 150 function words; Argamon,
Saric, and Stein (2003) used a set of 303 words; Zhao and Zobel (2005) used a set of 365
function words; 480 function words were proposed by Koppel and Schler (2003); another set
of 675 words was reported by Argamon, Whitelaw, Chase, Hota, Garg, and Levitan (2007).
A simple and very successful method to define a lexical feature set for authorship
attribution is to extract the most frequent words found in the available corpus (comprising all
the texts of the candidate authors). Then, a decision has to be made about the amount of the
frequent words that will be used as features. In the earlier studies, sets of at most 100 frequent
words were considered adequate to represent the style of an author (Burrows, 1987; Burrows,
1992). Another factor that affects the feature set size is the classification algorithm that will
be used since many algorithms overfit the training data when the dimensionality of the
problem increases. However, the availability of powerful machine learning algorithms able to
deal with thousands of features, like support vector machines (Joachims, 1998), enabled
researchers to increase the feature set size of this method. Koppel, Schler, and BonchekDokow
(2007) used the 250 most frequent words while Stamatatos (2006a) extracted the
1,000 most frequent words. On a larger scale, Madigan, et al., (2005) used all the words that
appear at least twice in the corpus. Note that the first dozens of most frequent words of a
corpus are usually dominated by closed class words (articles, prepositions etc.) After a few
hundred words, open class words (nouns, adjectives, verbs) are the majority. Hence, when the
dimensionality of this representation method increases, some content-specific words may also
be included in the feature set.
Despite the availability of a tokenizer, word-based features may require additional tools
for their extraction. This would involve from simple routines like conversion to lowercase to
more complex tools like stemmers (Sanderson & Guenter, 2006), lemmatizers (Tambouratzis,
Markantonatou, Hairetakis, Vassiliou, Carayannis, & Tambouratzis, 2004; Gamon, 2004), or
detectors of common homographic forms (Burrows, 2002). Another procedure used by van
Halteren (2007) is to transform words into an abstract form. For example, the Dutch word
‘waarmaken’ is transformed to ‘#L#6+/L/ken’, where the first L indicates low frequency, 6+ 
indicates the length of the token, the second L a lowercase token, and ‘ken’ are its last three
characters.
The bag-of-words approach provides a simple and efficient solution but disregards wordorder
(i.e., contextual) information. For example, the phrases ‘take on’, ‘the second take’ and
‘take a bath’ would just provide three occurrences of the word ‘take’. To take advantage of
contextual information, word n-grams (n contiguous words aka word collocations) have been
proposed as textual features (Peng, et al., 2004; Sanderson & Guenther, 2006; CoyotlMorales,
Villaseñor-Pineda, Montes-y-Gómez, & Rosso, 2006). However, the classification
accuracy achieved by word n-grams is not always better than individual word features
(Sanderson & Guenther, 2006; Coyotl-Morales, et al., 2006). The dimensionality of the
problem following this approach increases considerably with n to account for all the possible
combinations between words. Moreover, the representation produced by this approach is very
sparse, since most of the word combinations are not encountered in a given (especially short)
text making it very difficult to be handled effectively by a classification algorithm. Another
problem with word n-grams is that it is quite possible to capture content-specific information
rather than stylistic information (Gamon, 2004).
From another point of view, Koppel and Schler (2003) proposed various writing error
measures to capture the idiosyncrasies of an author’s style. To that end, they defined a set of
spelling errors (e.g., letter omissions and insertions) and formatting errors (e.g., all caps
words) and they proposed a methodology to extract such measures automatically using a spell
checker. Interestingly, human experts mainly use similar observations in order to attribute
authorship. However, the availability of accurate spell checkers is still problematic for many
natural languages. 
According to this family of measures, a text is viewed as a mere sequence of characters. That
way, various character-level measures can be defined, including alphabetic characters count,
digit characters count, uppercase and lowercase characters count, letter frequencies,
punctuation marks count, etc. (de Vel, et al., 2001; Zheng, et al., 2006). This type of
information is easily available for any natural language and corpus and it has been proven to
be quite useful to quantify the writing style (Grieve, 2007).
A more elaborate, although still computationally simplistic, approach is to extract
frequencies of n-grams on the character-level. For instance, the character 4-grams of the
beginning of this paragraph would be1
: |A_mo|, |_mor|, |more|, |ore_|, |re_e|, etc. This
approach is able to capture nuances of style including lexical information (e.g., |_in_|, |text|),
hints of contextual information (e.g., |in_t|), use of punctuation and capitalization, etc.
Another advantage of this representation is its ability to be tolerant to noise. In cases where
the texts in question are noisy containing grammatical errors or making strange use of
punctuation, as it usually happens in e-mails or online forum messages, the character n-gram
representation is not affected dramatically. For example, the words ‘simplistic’ and
‘simpilstc’ would produce many common character trigrams. On the other hand, these two
words would be considered different in a lexically-based representation. Note that in stylebased
text categorization such errors could be considered personal traits of the author (Koppel
& Schler, 2003). This information is also captured by character n-grams (e.g., in the
uncommon trigrams |stc| and |tc_|). Finally, for oriental languages where the tokenization
procedure is quite hard, character n-grams offer a suitable solution (Matsuura & Kanada,
2000). As can be seen in Table 1, the computational requirements of character n-gram
features are minimal. 
Note that, as with words, the most frequent character n-grams are the most important
features for stylistic purposes. The procedure of extracting the most frequent n-grams is
language-independent and requires no special tools. However, the dimensionality of this
representation is considerably increased in comparison to the word-based approach 
(Stamatatos, 2006a; Stamatatos, 2006b). This happens because character n-grams capture
redundant information (e.g., |and_|, |_and|) and many character n-grams are needed to
represent a single long word.
The application of this approach to authorship attribution has been proven quite
successful. Kjell (1994) first used character bigrams and trigrams to discriminate the
Federalist Papers. Forsyth and Holmes (1996) found that bigrams and character n-grams of
variable-length performed better than lexical features in several text classification tasks
including authorship attribution. Peng, Shuurmans, Keselj, & Wang (2003), Keselj et al.
(2003), and Stamatatos (2006b) reported very good results using character n-gram
information. Moreover, one of the best performing algorithms in an authorship attribution
competition organized in 2004 was also based on a character n-gram representation (Juola,
2004; Juola, 2006). Likewise, a recent comparison of different lexical and character features
on the same evaluation corpora (Grieve, 2007) showed that character n-grams were the most
effective measures (outperformed in the specific experiments only by a combination of
frequent words and punctuation marks).
An important issue of the character n-gram approach is the definition of n, that is, how
long should the strings be. A large n would better capture lexical and contextual information
but it would also better capture thematic information. Furthermore, a large n would increase
substantially the dimensionality of the representation (producing hundreds of thousands of
features). On the other hand, a small n (2 or 3) would be able to represent sub-word (syllablelike)
information but it would not be adequate for representing the contextual information. It
has to be underlined that the selection of the best n value is a language-dependent procedure
since certain natural languages (e.g., Greek, German) tend to have long words in comparison
to English. Therefore, probably a larger n value would be more appropriate for such
languages in comparison to the optimal n value for English. The problem of defining a fixed
value for n can be avoided by the extraction of n-grams of variable-length (Forsyth &
Holmes, 1996; Houvardas & Stamatatos, 2006). Sanderson and Guenter (2006) described the
use of several sequence kernels based on character n-grams of variable-length and the best
results for short English texts were achieved when examining sequences of up to 4-grams.
Moreover, various Markov models of variable order have been proposed for handling
character-level information (Khmelev & Teahan, 2003a; Marton, et al., 2005). Finally, Zhang
and Lee (2006) constructed a suffix tree representing all possible character n-grams of
variable-length and then extracted groups of character n-grams as features.
A quite particular case of using character information is the compression-based
approaches (Benedetto, Caglioti, & Loreto, 2002; Khmelev & Teahan, 2003a; Marton, et al.,
2005). The main idea is to use the compression model acquired from one text to compress
another text, usually based on off-the-shelf compression programs. If the two texts are written
by the same author, the resulting bit-wise size of the compressed file will be relatively low.
Such methods do not require a concrete representation of text and the classification algorithm
incorporates the quantification of textual properties. However, the compression models that
describe the characteristics of the texts are usually based on repetitions of character sequences
and, as a result, they can capture sub-word and contextual information. In that sense, they can
be considered as character-based methods. 
A more elaborate text representation method is to employ syntactic information. The idea is
that authors tend to use similar syntactic patterns unconsciously. Therefore, syntactic
information is considered more reliable authorial fingerprint in comparison to lexical
information. Moreover, the success of function words in representing style indicates the
usefulness of syntactic information since they are usually encountered in certain syntactic
structures. On the other hand, this type of information requires robust and accurate NLP tools
able to perform syntactic analysis of texts. This fact means that the syntactic measure
extraction is a language-dependent procedure since it relies on the availability of a parser able 
In a similar framework, tools that perform partial parsing can be used to provide
syntactic features of varying complexity (Luyckx & Daelemans, 2005; Uzuner & Katz, 2005;
Hirst & Feiguina, 2007). Partial parsing is between text chunking and full parsing and can
handle unrestricted text with relatively high accuracy. Hirst and Feiguina (2007) transformed
the output of a partial parser into an ordered stream of syntactic labels, for instance the
analysis of the phrase ‘a simple example’ would produce the following stream of labels: 
It should be clear by now, the more detailed the text analysis required for extracting
stylometric features, the less accurate (and the more noisy) the produced measures. NLP tools
can be applied successfully to low-level tasks, such as sentence splitting, POS tagging, text
chunking, partial parsing, so relevant features would be measured accurately and the noise in
the corresponding datasets remains low. On the other hand, more complicated tasks such as
full syntactic parsing, semantic analysis, or pragmatic analysis cannot yet be handled
adequately by current NLP technology for unrestricted text. As a result, very few attempts
have been made to exploit high-level features for stylometric purposes.
Gamon (2004) used a tool able to produce semantic dependency graphs but he did not
provide information about the accuracy of this tool. Two kinds of information were then
extracted: binary semantic features and semantic modification relations. The former
concerned number and person of nouns, tense and aspect of verbs, etc. The latter described
the syntactic and semantic relations between a node of the graph and its daughters (e.g., a
nominal node with a nominal modifier indicating location). Reported results showed that
semantic information when combined with lexical and syntactic information improved the
classification accuracy.
McCarthy, Lewis, Dufty, and McNamara (2006) described another approach to extract
semantic measures. Based on WordNet (Fellbaum, 1998) they estimated information about
synonyms and hypernyms of the words, as well as the identification of causal verbs.
Moreover, they applied latent semantic analysis (Deerwester, Dumais, Furnas, Landauer, &
Harshman, 1990) to lexical features in order to detect semantic similarities between words
automatically. However, there was no detailed description of the features and the evaluation
procedure did not clarify the contribution of semantic information in the classification model.
Perhaps the most important method of exploiting semantic information so far was
described by Argamon, et al. (2007). Inspired by the theory of Systemic Functional Grammar
(SFG) (Halliday, 1994) they defined a set of functional features that associate certain words
or phrases with semantic information. In more detail, in SFG the ‘CONJUNCTION’ scheme